---
title: "Multiple regression with interaction terms"
author: "Luis Castro-de-Araujo ^[Post-doc T32. luis.araujo@vcuhealth.org  \n]"
date: 11/07/2022
institute: Virginia Institute for Psychiatric and Behavioral Genetics 
csl: style/chicago-fullnote-bibliography.csl
suppress-bibliography: true
bibliography: /home/luis/Documents/library.bib
lang: en-AU
toc: yes 
output: beamer_presentation
classoption: "aspectratio=169"
theme: metropolis
header-includes: |
 \usepackage{navigator}
 \embeddedfile{project}{2022-presentation.Rmd}
  \usepackage{appendixnumberbeamer}
  \usepackage{roboto}
 \usepackage[font={footnotesize}]{caption}
 \setbeamerfont{footnote}{size=\tiny}
 \metroset{sectionpage=progressbar, titleformat=smallcaps}
 \setbeamercolor{itemize item}{fg=vcuyellow}
 \setbeamercolor{itemize subitem}{fg=vcuyellow}
 \setbeamercolor{itemize subsubitem}{fg=vcuyellow}
  \setbeamertemplate{itemize item}[circle]
  \setbeamertemplate{itemize subitem}[square]
  \setbeamertemplate{itemize subsubitem}[triangle]
  \titlegraphic{%
  \hfill\includegraphics[width=3cm,height=1.6cm,keepaspectratio]{style/VCU-Logo.png}}
  \definecolor{vcuyellow}{HTML}{FDBD10}
 \setbeamercolor{palette primary}{fg=vcuyellow, bg=black}
  \setbeamercolor{frametitle}{fg=vcuyellow, bg=black}
  \setbeamercolor{titlelike}{fg=black}
  \setbeamercolor{titlepage}{fg=black, bg=vcuyellow}
  \setbeamercolor{footnote}{bg=black}
  \setbeamercolor{normal text}{fg=black}
  \setbeamercolor{block title}{fg=black, bg=vcuyellow!40!white}
  \setbeamercolor{alerted text}{fg=red}
  \setbeamercolor{example text}{fg=black}
  \setbeamercolor{title separator}{fg = vcuyellow, bg=vcuyellow}
 \setbeamercolor{progress bar}{bg=white, fg=vcuyellow}
 \setbeamercolor{progress bar in head/foot}{bg=white, fg=vcuyellow}
  \setbeamercolor{progress bar in section page}{bg=white, fg=vcuyellow}
 \makeatletter
 \newsavebox{\mybox}
 \setbeamertemplate{frametitle}{%
  \nointerlineskip%
  \savebox{\mybox}{%
      \begin{beamercolorbox}[%
          wd=\paperwidth,%
          sep=0pt,%
          leftskip=\metropolis@frametitle@padding,%
          rightskip=\metropolis@frametitle@padding,%
        ]{frametitle}%
      \metropolis@frametitlestrut@start\insertframetitle\metropolis@frametitlestrut@end%
      \end{beamercolorbox}%
    }
  \begin{beamercolorbox}[%
      wd=\paperwidth,%
      sep=0pt,%
      leftskip=\metropolis@frametitle@padding,%
      rightskip=\metropolis@frametitle@padding,%
    ]{frametitle}%
  \metropolis@frametitlestrut@start\insertframetitle\metropolis@frametitlestrut@end%
  \hfill%
  \raisebox{-\metropolis@frametitle@padding}{\includegraphics[height=\dimexpr\ht\mybox+\metropolis@frametitle@padding\relax]{style/VCU-Logo.png}}%
    \hspace*{-\metropolis@frametitle@padding}
  \end{beamercolorbox}%
 }
 \makeatother
  \makeatletter
  \pretocmd{\beamer@reseteecodes}{%
    \ifbool{metropolis@standout}{
      \endgroup
      \boolfalse{metropolis@standout}
    }{}
  }{}{}
  \makeatother
---




\section{Multiple regression recap}


# Multiple Linear Regression

-  Extension of the simple linear regression model to two or more independent variables


$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p + \epsilon$$

-  Expression = Baseline + Age + Tissue + Sex + Error

- Partial Regression CoefÔ¨Åcients: ieffect on the  dependent variable when increasing the *i*th independent variable by 1 unit, **holding all other predictors constant**

# Categorical Independent Variables

-  Qualitative variables are easily incorporated in regression  framework through ***dummy variables***

- Simple example: sex can be coded as 0/1

- What if my categorical variable contains three levels:

\[
\begin{aligned}
& \mathrm{x}_{1}=\left\{\begin{array}{l}0 \text { if } \mathrm{AA} 
                  \\1 \text { if } \mathrm{AG} 
                  \\2 \text { if } \mathrm{GG}\end{array}\right. 
\end{aligned}
\]

# Categorical Independent Variables


:::::::::::::: {.columns}
::: {.column width="50%"}

-  Previous coding would result in ***colinearity***

-  Solution is to set up a series of dummy variable. 

- for k levels you need k-1 dummy variables

\[
\begin{aligned}
& \mathrm{x}_{1}=\left\{\begin{array}{l}1 \text { if } \mathrm{AA} \\0 \text { otherwise }\end{array}\right. \\
& x_{2}=\left\{\begin{array}{l}1 \text { if } A G \\0 \text { otherwise }\end{array}\right.
\end{aligned}
\]

:::
::: {.column width="50%"}


|  |  x1 |  x2 |
|:--:|:--:|:--:|
|  AA |  1 |  0 |
|  AG |  0 |  1 |
|  GG |  0 |  0 |


:::
::::::::::::::


# Assumptions

Validity
: Does the data we're modeling matches to the problem we're actually trying to solve?

Representativeness
: Is the sample data used to train the regression model representative of the population to which it will be applied?

Additivity and Linearity
: The deterministic component of a regression model is a linear function of the separate predictors: $y=B_0 + B_1x_1 + ... + B_px_p$


Independence of Errors
: The errors from our model are independent.

Homoscedasticity
: The errors from our model have equal variance.

Normality of Errors
: The errors from our model are normally distributed.



# Multivariate regression

:::::::::::::: {.columns}
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/interpret-3} \end{center}

house price=-27154+757*sqft+51867*pool

- In our example, we model home prices as a function of both the size of the house (sqft) and whether or not it has a pool

:::
::: {.column width="50%"}

\tiny

- intercept: -$27,154,  the predicted average housing price for houses with all x~i~ = 0. Or the cost of houses with no pools and a square-footage of zero.

- coefficient of pool: $51,867, average expected price difference in houses of the same size (in sqft) if they do or do not have a pool. In other words, we expect, on average, houses of the same size to cost $51,867 more if they have a pool than if they do not.

- coefficient of sqft: $757,  average expected price difference in housing price for houses that have the same value of pool but differ in size by one square-foot.

- We assume the same slope for sqft.Hence, two lines. This isn't always a valid assumption to make.
:::
::::::::::::::


# Interpretation 4


:::::::::::::: {.columns}
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/interpret-4} \end{center}

\tiny

- house price=-70296+899*sqft+217111*pool-347*(sqft:pool)
- If we believe that the slope for sqft should differ between houses that do have pools and houses that do not, we can add an interaction term to our model, (sqft:pool).

:::
::: {.column width="50%"}

\tiny

- interaction term: -$347, represents the difference in the slope for sqft, comparing houses that do and do not have pools. Visually, this represents the difference between the slopes of the two lines.

- intercept: -$70,296, represents the predicted housing price for houses with no pools and a square-footage of zero.

- coefficient of pool: $217,111, represents the average expected difference in houses of the same size (0 sqft) that differed in whether or not they had a pool. (It's not super useful since we don't have houses with 0 square-feet).

- coefficient of sqft: $899, represents the average expected difference in housing price for houses that do not have a pool (pool= 0) but differ in size by one square-foot.
:::
::::::::::::::


#


\section{Interaction terms}


# What is an Interaction?

- An interaction is a predictor that is
some combination of the other
predictors.

# Constructing an interaction


- Interactions are often the product of two or
more predictors.
-  Can be written as,

$$Y = \beta_0 + \beta_1X_1 + \beta_2X_2 + \beta_3X_1X_2 + \epsilon$$

# Conditional vs. marginal effects

- Conditional effects: the effect of a predictor on the response, holding all other predictors constant.

- Marginal effects: the effect of a predictor on the response, averaged over all values of the other predictors.


# Conditional vs. marginal effects

- If the conditional effects of X1 on Y at
different levels of X2 are all the same then
there is no interaction.

# Interpreting interaction effects

\small
|Parameter | Meaning | Where people (used to) go awry |
|:----|----------------|--------------------------------|
| $\beta_0$ | Expected value of the DV when X1 and X2 ==0 | People get this  |
| $\beta_1$ | Effect of X1 when X2 == 0 | Not marginal effects!  |
| $\beta_2$ | Effect of X2 when X1 == 0 | Not marginal effects!  |
| $\beta_3$ | The addition to the conditional effect when both X! and X2 are 1 | People just look at the significance of the interaction parameter and do not calculate the underlying marginal or conditional effects or standard errors |

# In the past it was common to see standard errors wrongly calculated

- A common mistake that people make when
interpreting interaction models is using the
wrong standard errors.
- The standard errors that are printed in every
regression table are the positive square roots of
the diagonal elements of the variance-
covariance matrix of $\beta$
- This does not matter anymore because of `margins()`


# 

\section{Extracting the marginal effects}






# Having fun with mtcars - regression of speed on wt*cyl 

:::::::::::::: {.columns}
::: {.column width="50%"}

\tiny


```r
fit <- glm(qsec ~ wt*cyl, data = mtcars)
summary(fit)
```

:::
::: {.column width="50%"}

\tiny


```

Call:
glm(formula = qsec ~ wt * cyl, data = mtcars)

Deviance Residuals: 
    Min       1Q   Median       3Q      Max  
-2.1966  -0.8373   0.0499   0.8158   2.1398  

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   16.726      3.118    5.36  0.00001 ***
wt             2.858      1.180    2.42    0.022 *  
cyl           -0.542      0.511   -1.06    0.298    
wt:cyl        -0.222      0.167   -1.33    0.193    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 1.45)

    Null deviance: 98.988  on 31  degrees of freedom
Residual deviance: 40.636  on 28  degrees of freedom
AIC: 108.5

Number of Fisher Scoring iterations: 2
```

:::
::::::::::::::

# Regression of mpg on wt*cyl

:::::::::::::: {.columns}
::: {.column width="50%"}

\tiny


```r
fit <- glm(mpg ~ wt*cyl, data = mtcars)
summary(fit)
```

:::
::: {.column width="50%"}

\tiny


```

Call:
glm(formula = mpg ~ wt * cyl, data = mtcars)

Deviance Residuals: 
   Min      1Q  Median      3Q     Max  
-4.229  -1.350  -0.504   1.465   5.234  

Coefficients:
            Estimate Std. Error t value     Pr(>|t|)    
(Intercept)   54.307      6.128    8.86 0.0000000013 ***
wt            -8.656      2.320   -3.73      0.00086 ***
cyl           -3.803      1.005   -3.78      0.00075 ***
wt:cyl         0.808      0.327    2.47      0.01988 *  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for gaussian family taken to be 5.61)

    Null deviance: 1126.05  on 31  degrees of freedom
Residual deviance:  156.98  on 28  degrees of freedom
AIC: 151.7

Number of Fisher Scoring iterations: 2
```
:::
::::::::::::::

# Regression of mpg on wt*cyl

:::::::::::::: {.columns}
::: {.column width="50%"}

\tiny


```r
pred <- ggpredict(fit, terms = c("wt", "cyl"))
plot(pred, add.data = TRUE)+
  theme_luis()
```

:::
::: {.column width="50%"}


![](../graphs/unnamed-chunk-8-1.pdf)<!-- --> 
:::
::::::::::::::


# Now, from what point the slope becomes non signficant?

:::::::::::::: {.columns}
::: {.column width="50%"}
## Changing to mpg ~ hp + wt

\tiny


```r
fit <- glm(mpg ~ hp*wt, data = mtcars)
pred <- ggpredict(fit, terms = c("hp", "wt"))

plot(pred, add.data = TRUE) +
  theme_luis()
```


:::
::: {.column width="50%"}

![](../graphs/unnamed-chunk-10-1.pdf)<!-- --> 

:::
::::::::::::::

# Now, from what point the slope becomes non signficant?

:::::::::::::: {.columns}
::: {.column width="50%"}

## JOHNSON-NEYMAN INTERVAL

\tiny


```r
johnson_neyman(fit, wt, hp , plot = TRUE)
```



:::
::: {.column width="50%"}



```
JOHNSON-NEYMAN INTERVAL 

When hp is OUTSIDE the interval [236.48, 471.76], the slope of wt is p <
.05.

Note: The range of observed values of hp is [52.00, 335.00]
```

![](../graphs/unnamed-chunk-12-1.pdf)<!-- --> 

:::
::::::::::::::


# Now, from what point the slope becomes non signficant?

:::::::::::::: {.columns}
::: {.column width="50%"}

## JOHNSON-NEYMAN INTERVAL - Overlayed over data 

\tiny


```r
fit <- glm(mpg ~ hp*wt, data = mtcars)
pred <- ggpredict(fit, terms = c("hp", "wt"))
jn<-johnson_neyman(fit, wt, hp , plot = TRUE)
jn_bound<-as.numeric(jn$bounds[1])  

plot(pred, add.data=T) + 
  geom_vline(xintercept = jn_bound, linetype = "dashed", 
             color = "red")+
  theme_luis()
```



:::
::: {.column width="50%"}


![](../graphs/unnamed-chunk-14-1.pdf)<!-- --> 

:::
::::::::::::::



# Conclusion

- We started reviwing multiple regression
- Then discussed the syntax and interpretation of parameters when an interaction term is included
- Finally, we discussed how to extract the marginal effects of the interaction term
- Luckly the package margins() makes this extremely simple, thus lectures on interaction became much shorter than it used to be (if you want a hands on approach).



# Acknowledgements


:::::::::::::: {.columns}
::: {.column width="50%"}

## Team

- Charles Gardner (2015) 
- Brad Verhulst (2013)
- Joshua Pritkin. 
- Rob Kirkpatrick. 

\vspace{3mm}
- Michael C Neale.  

- NIH grant no R01 DA049867 and 5T32MH-020030 


:::
::: {.column width="50%"}


## Contact 


\begin{center}\includegraphics[width=0.8\linewidth]{../graphs/qr-twitter-1} \end{center}


:::
::::::::::::::

\appendix

#  {.standout}


- Thank you

