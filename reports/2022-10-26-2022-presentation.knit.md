---
title: "Multiple regression & Gauss-Markov Theorem"
author: "Luis Castro-de-Araujo ^[Post-doc T32. luis.araujo@vcuhealth.org  \n]"
date: 11/07/2022
institute: Virginia Institute for Psychiatric and Behavioral Genetics 
csl: style/chicago-fullnote-bibliography.csl
suppress-bibliography: true
bibliography: /home/luis/Documents/library.bib
lang: en-AU
toc: yes 
output: beamer_presentation
classoption: "aspectratio=169"
theme: metropolis
header-includes: |
 \usepackage{navigator}
 \embeddedfile{project}{2022-presentation.Rmd}
  \usepackage{appendixnumberbeamer}
  \usepackage{roboto}
 \usepackage[font={footnotesize}]{caption}
 \setbeamerfont{footnote}{size=\tiny}
 \metroset{sectionpage=progressbar, titleformat=smallcaps}
 \setbeamercolor{itemize item}{fg=vcuyellow}
 \setbeamercolor{itemize subitem}{fg=vcuyellow}
 \setbeamercolor{itemize subsubitem}{fg=vcuyellow}
  \setbeamertemplate{itemize item}[circle]
  \setbeamertemplate{itemize subitem}[square]
  \setbeamertemplate{itemize subsubitem}[triangle]
  \titlegraphic{%
  \hfill\includegraphics[width=3cm,height=1.6cm,keepaspectratio]{style/VCU-Logo.png}}
  \definecolor{vcuyellow}{HTML}{FDBD10}
 \setbeamercolor{palette primary}{fg=vcuyellow, bg=black}
  \setbeamercolor{frametitle}{fg=vcuyellow, bg=black}
  \setbeamercolor{titlelike}{fg=black}
  \setbeamercolor{titlepage}{fg=black, bg=vcuyellow}
  \setbeamercolor{footnote}{bg=black}
  \setbeamercolor{normal text}{fg=black}
  \setbeamercolor{block title}{fg=black, bg=vcuyellow!40!white}
  \setbeamercolor{alerted text}{fg=red}
  \setbeamercolor{example text}{fg=black}
  \setbeamercolor{title separator}{fg = vcuyellow, bg=vcuyellow}
 \setbeamercolor{progress bar}{bg=white, fg=vcuyellow}
 \setbeamercolor{progress bar in head/foot}{bg=white, fg=vcuyellow}
  \setbeamercolor{progress bar in section page}{bg=white, fg=vcuyellow}
 \makeatletter
 \newsavebox{\mybox}
 \setbeamertemplate{frametitle}{%
  \nointerlineskip%
  \savebox{\mybox}{%
      \begin{beamercolorbox}[%
          wd=\paperwidth,%
          sep=0pt,%
          leftskip=\metropolis@frametitle@padding,%
          rightskip=\metropolis@frametitle@padding,%
        ]{frametitle}%
      \metropolis@frametitlestrut@start\insertframetitle\metropolis@frametitlestrut@end%
      \end{beamercolorbox}%
    }
  \begin{beamercolorbox}[%
      wd=\paperwidth,%
      sep=0pt,%
      leftskip=\metropolis@frametitle@padding,%
      rightskip=\metropolis@frametitle@padding,%
    ]{frametitle}%
  \metropolis@frametitlestrut@start\insertframetitle\metropolis@frametitlestrut@end%
  \hfill%
  \raisebox{-\metropolis@frametitle@padding}{\includegraphics[height=\dimexpr\ht\mybox+\metropolis@frametitle@padding\relax]{style/VCU-Logo.png}}%
    \hspace*{-\metropolis@frametitle@padding}
  \end{beamercolorbox}%
 }
 \makeatother
  \makeatletter
  \pretocmd{\beamer@reseteecodes}{%
    \ifbool{metropolis@standout}{
      \endgroup
      \boolfalse{metropolis@standout}
    }{}
  }{}{}
  \makeatother
---




\section{Linear regression recap}

# Regression


-  Develop basic concepts of linear regression from a probabilistic framework
-  Estimating parameters and hypothesis testing with linear models

# Regression 

- Technique used for the modeling and analysis of numerical data [@akeyRegression2020]
- Exploits the relationship between two or more variables so that we can gain information about one of them through knowing values of the other
- Regression can be used for prediction, estimation, hypothesis testing, and modeling causal relationships

# {.standout}

- It is all about describing relationships between variables

# The culprit


:::::::::::::: {.columns}
::: {.column width="50%"}

## Bio

- 30 April 1777 -  23 April 1855
- Worked in the theorem around 1794
- 17 years old! [@CarlFriedrichGauss2022]

:::
::: {.column width="50%"}


## Carl Friedrich Gauss


\includegraphics[width=1\linewidth]{../graphs/gauss} 

:::
::::::::::::::

# Before him, description of relationship was not systematic

:::::::::::::: {.columns}
::: {.column width="50%"}

\small


```r
set.seed(42)
x <- rnorm(100)
y <- 2 * x + rnorm(100, sd = 0.8)
plot(x, y, xlab = "x", ylab = "y")
```

:::
::: {.column width="50%"}

![](../graphs/unnamed-chunk-3-1.pdf)<!-- --> 
:::
::::::::::::::


# Regression Lingo [@akeyRegression2020]


$$Y = X1 + X2 + X3$$

| Left of expression | Right of expression |
|------|------------|
| Dependent Variable | Independent Variable |
| Outcome Variable | Predictor Variable |
| Response Variable | Explanatory Variable |


# Why Linear Regression? 

- Suppose we want to model the dependent variable Y in terms of three predictors, X1, X2, X3 [@akeyRegression2020]



$$Y = f(X1, X2, X3)$$

- We usually have to assume that it has some restricted form, such as linear + error

$$Y = X1 + X2 + X3 + \epsilon$$


# Linear Regression is a Probabilistic Model

:::::::::::::: {.columns}
::: {.column width="50%"}

- Much of mathematics is devoted to studying variables that are deterministically related to one another [@akeyRegression2020]



$$y = \beta_0 + \beta_1x$$

- But we're interested in understanding the relationship between variables related in a nondeterministic fashion.


:::
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/ari} \end{center}

:::
::::::::::::::


# A Linear Probabilistic Model 

-  There exists parameters $\beta_0$, $\beta_1$, and $\sigma^2$ , such that for
any ﬁxed value of the independent variable x, the dependent variable is
related to x through the model equation [@akeyRegression2020]

$$y = \beta_0 + \beta_1x + \epsilon$$

-  The error term $\epsilon$ is a random variable with mean 0 and constant variance $\sigma^2$

# A substantive example 

## From a bad model [@wilberLinearRegression]


\begin{center}\includegraphics[width=1\linewidth]{../graphs/predict-1} \end{center}

# In other words 


\begin{center}\includegraphics[width=1\linewidth]{../graphs/predict-2} \end{center}

# In other words 


\begin{center}\includegraphics[width=1\linewidth]{../graphs/predict-3} \end{center}

# In other words 

## To the best **possible** model [@wilberLinearRegression]



\begin{center}\includegraphics[width=1\linewidth]{../graphs/prediction-4} \end{center}



# Implications

- The **expected** value of Y is a linear function of X, but for
ﬁxed x, the variable Y differs from its expected value by a random
amount [@wilberLinearRegression]

- Formally, let x\* denote a particular value of the independent variable x, then our linear probabilistic model says:


$$E(Y|x^*) = \mu_{Y|x^*}\text{ = mean value of Y when x is x*}$$

$$V(Y|x^*) = \sigma^2_{Y|x^*}\text{= variance of Y when x is x*}$$
  

# Graphical Interpretation


\includegraphics[width=4.83in]{../graphs/regression} 

- For example, if x = height and y = weight then $\mu_{Y|x^*}=60$ is the average weight for all individuals 60 inches tall in the population 


# Estimating Model Parameters


:::::::::::::: {.columns}
::: {.column width="50%"}

- Point estimates of $\hat{\beta_0}$ and $\hat{\beta_1}$ are obtained by the principle of least squares

$$f(\beta_0, \beta_1) = \sum_{i=1}^n (y_i - \beta_0 - \beta_1x_i)^2$$



- $\hat{\beta_0} = \hat{y} - \hat{\beta_1}\hat{x}$


:::
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/estimating} \end{center}
:::
::::::::::::::

# Predicted and Residual Values

:::::::::::::: {.columns}
::: {.column width="50%"}

\tiny 

- **Predicted** or fitted, values of y predicted by the least-squares regression line obtained by plugging in x1,x2,…,xn into the estimated regression line

$$\hat{y_1} = \hat{\beta_0} + \hat{\beta_1}x_1$$
$$\hat{y_2} = \hat{\beta_0} + \hat{\beta_1}x_2$$

- **Residuals** are the deviations of observed and predicted values

$$e_1 = y_1 - \hat{y_1}$$
$$e_2 = y_2 - \hat{y_2}$$ 



:::
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/predicted} \end{center}


:::
::::::::::::::



# Residuals Are Useful!

- They allow us to calculate the error sum of squares (SSE):

$$SSE = \sum_{i=1}^n (e_i)^2 = \sum_{i=1}^n (y_i - \hat{y_i})^2$$

- Which in turn allows us to estimate $\sigma^2$:

$$\hat{\sigma^2} = \frac{SSE}{n-2}$$

- As well as the **coefficient of determination**:

$$R^2 = 1 - \frac{SSE}{SST};  SST = \sum_{i=1}^n (y_i - \bar{y})^2$$



# Binary variable

:::::::::::::: {.columns}
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/interpret-1} \end{center}

$\begin{aligned} \text{house price} = 172893 + 241582 * pool \end{aligned}$

- summarizes the difference in average housing prices between houses with and without  pools 
:::
::: {.column width="50%"}

\small


- The intercept, $172,893, is the average predicted price for houses that do not have swimming pools.

- To find the average price predicted price for houses with pools, we simply plug in pool=1 to obtain $172,893 + $241,582 * 1 = $414,475.

- The difference between these two subpopulation means is equal to the coefficient on pool. Houses with pools cost $241,582 more on average than houses that do not have pools.

:::
::::::::::::::

# One continuous variable 

:::::::::::::: {.columns}
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/interpret-2} \end{center}


house price=-39591+742*sqft

- summarizes the average house prices across differently sized houses as measured in square feet.

:::
::: {.column width="50%"}

\small


- The coefficient, $742, represents the average difference in housing price for one-unit difference in the square-footage of the house. In other words, we expect each additional square-foot, on average, to raise the price of a house by $742.

- The intercept, -$39,591, represents the predicted housing price for houses with sqft = 0, that is, it represents the average price of a zero square-foot house. Because this value doesn't make much intuitive sense, it's common for models to be transformed and standardized before carrying out a regression model.

:::
::::::::::::::

#

\section{Multiple regression}


# Multiple Linear Regression

-  Extension of the simple linear regression model to two or more independent variables


$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \dots + \beta_px_p + \epsilon$$

-  Expression = Baseline + Age + Tissue + Sex + Error

- Partial Regression Coefﬁcients: ieffect on the  dependent variable when increasing the *i*th independent variable by 1 unit, **holding all other predictors constant**

# Categorical Independent Variables

-  Qualitative variables are easily incorporated in regression  framework through ***dummy variables***

- Simple example: sex can be coded as 0/1

- What if my categorical variable contains three levels:

\[
\begin{aligned}
& \mathrm{x}_{1}=\left\{\begin{array}{l}0 \text { if } \mathrm{AA} 
                  \\1 \text { if } \mathrm{AG} 
                  \\2 \text { if } \mathrm{GG}\end{array}\right. 
\end{aligned}
\]

# Categorical Independent Variables


:::::::::::::: {.columns}
::: {.column width="50%"}

-  Previous coding would result in ***colinearity***

-  Solution is to set up a series of dummy variable. 

- for k levels you need k-1 dummy variables

\[
\begin{aligned}
& \mathrm{x}_{1}=\left\{\begin{array}{l}1 \text { if } \mathrm{AA} \\0 \text { otherwise }\end{array}\right. \\
& x_{2}=\left\{\begin{array}{l}1 \text { if } A G \\0 \text { otherwise }\end{array}\right.
\end{aligned}
\]

:::
::: {.column width="50%"}


|  |  x1 |  x2 |
|:--:|:--:|:--:|
|  AA |  1 |  0 |
|  AG |  0 |  1 |
|  GG |  0 |  0 |


:::
::::::::::::::


# Assumptions

Validity
: Does the data we're modeling matches to the problem we're actually trying to solve?

Representativeness
: Is the sample data used to train the regression model representative of the population to which it will be applied?

Additivity and Linearity
: The deterministic component of a regression model is a linear function of the separate predictors: $y=B_0 + B_1x_1 + ... + B_px_p$


Independence of Errors
: The errors from our model are independent.

Homoscedasticity
: The errors from our model have equal variance.

Normality of Errors
: The errors from our model are normally distributed.



# Multivariate regression

:::::::::::::: {.columns}
::: {.column width="50%"}


\begin{center}\includegraphics[width=1\linewidth]{../graphs/interpret-3} \end{center}

house price=-27154+757*sqft+51867*pool

- In our example, we model home prices as a function of both the size of the house (sqft) and whether or not it has a pool

:::
::: {.column width="50%"}

\tiny

- intercept: -$27,154,  the predicted average housing price for houses with all x~i~ = 0. Or the cost of houses with no pools and a square-footage of zero.

- coefficient of pool: $51,867, average expected price difference in houses of the same size (in sqft) if they do or do not have a pool. In other words, we expect, on average, houses of the same size to cost $51,867 more if they have a pool than if they do not.

- coefficient of sqft: $757,  average expected price difference in housing price for houses that have the same value of pool but differ in size by one square-foot.

- We assume the same slope for sqft.Hence, two lines. This isn't always a valid assumption to make.
:::
::::::::::::::


\section{Gauss-Markov Theorem}


# Gauss-Markov Theorem

- Provided  all previous assumptions hold
- It is possible to prove that OLS is precise and optimal in a sense
- Which sense? 

## Best Linear Unbiased Estimator (BLUE)

1. The parameters are *linear*
2. The parameters are *unbiased*
3. The parameters are *efficient*. In other words, they have the least variance of all unbiased linear estimators, *best*.


# The regression model again


\begin{equation}
\begin{aligned}
&Y_1=B_1+B_2 X_{21}+B_3 X_{31}+\cdots+B_k X_{k 1}+u_1 \\
&Y_2=B_1+B_2 X_{22}+B_3 X_{32}+\cdots+B_k X_{k 2}+u_2 \\
&\vdots \\
&Y_n=B_1+B_2 X_{2 n}+B_3 X_{3 n}+\cdots+B_k X_{k n}+u_n
\end{aligned}
\end{equation}
where:

- $y$ is an $N \times 1$ vector of observations of the output variable ($N$ is the sample size);
- X is an $N \times K$ matrix of inputs (K is the number of inputs for each observation);
- B is a $K \times 1$ vector of regression coefficients;
- u is an $N	\times 1$ vector of errors. [@tabogaGaussMarkovTheorem2021]
 

# The OLS estimator of $\beta$ is

$$\hat{\beta} = (X'X)^{-1}X'y$$

We assume that:

- X has full-rank (as a consequence, $X'X$ is invertible, and $\widehat{\beta}$ is well-defined);
- $\epsilon$ is a random vector with mean zero and covariance matrix $\sigma^2I$ (where $\sigma^2$ is the variance of the errors);
- $\epsilon$ is independent of X (i.e., $E(\epsilon | X) = 0$).[@tabogaGaussMarkovTheorem2021]

\alert{Proof not shown} [@gujaratiClassicalLinearRegression2019]

#   preciso declarar as pressuposiões com mais cuidado

Assumption 1: The regression model is linear in the parameters as in Equation (1.1); it may or may not be linear in the variables, the Ys and Xs.
Assumption 2: The regressors are assumed fixed, or nonstochastic, in the sense that their values are fixed in repeated sampling. However, if the regressors are stochastic, we assume that each regressor is independent of the error term or at least uncorrelated with it. We will discuss this assumption in more detail in Chapter 6.
Assumption 3: Given the values of the X variables, the expected, or mean, value of the error term ui is 0.
Assumption 4: The variance of each ui, given the values of X, is constant or homoscedastic (i.e., of equal variance). 

$$var(u_i | X) = \sigma^2
= E(uu')
= \sigma^2I$$
Assumption 5: There is no correlation between error terms belonging to two different observations.

$$cov(u_i, u_j | X) = 0$$

Assumption 6: There is no perfect linear relationship among the X variables. This is the assumption of no multicollinearity. 

# Homsc

\includegraphics[width=2.33in]{../graphs/homosc} 

# Not matrix



# As matrix

\begin{equation}
\left[\begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{array}\right]=\left[\begin{array}{cc}
1 & x_1 \\
1 & x_2 \\
\vdots & \vdots \\
1 & x_n
\end{array}\right]\left[\begin{array}{c}
\beta_0 \\
\beta_1
\end{array}\right]+\left[\begin{array}{c}
\varepsilon_1 \\
\varepsilon_2 \\
\vdots \\
\varepsilon_n
\end{array}\right]
\end{equation}

# As matrix

\begin{equation}
E\left[\begin{array}{l}
u_1 \\
u_2 \\
u_3 \\
\vdots \\
u_n
\end{array}\right]=\left[\begin{array}{l}
E\left(u_1\right) \\
E\left(u_2\right) \\
E\left(u_3\right) \\
\vdots \\
E\left(u_n\right)
\end{array}\right]=\left[\begin{array}{l}
0 \\
0 \\
0 \\
\vdots \\
0
\end{array}\right]
\end{equation}


# OLS is linear 

$$\hat{\beta} = (X'X)^{-1}X'y$$

First of all, note that $\widehat{\beta}$ is linear in $y$. In fact, $\widehat{\beta}$ is the product between the $K	\times N$ matrix $(X'X)^{-1}X'$ and $y$, and matrix multiplication is a linear operation. [@tabogaGaussMarkovTheorem2021]


# OLS is unbiased
:::::::::::::: {.columns}
::: {.column width="50%"}

$$
\begin{aligned}
b &=(X'X)^{-1} X'y \\
&=(X'X)^{-1} X'[X B+u], \text { substituting for } y \\
&=(X'X)^{-1} X'X B+(X'X)^{-1} X'u \\
&=B+(X'X)^{-1} X'u
\end{aligned}
$$

Now,

$$
\begin{aligned}
E(b)&=B+(X / X)^{-1} X^{\prime} E(u) \\
&=B
\end{aligned}
$$

:::
::: {.column width="50%"}


In words, the expected value of b is equal to B, thus proving that b is unbiased. (Recall the definition of unbiased estimator.) Note that $E(u|X) = 0$ by assumption.


:::
::::::::::::::



# Have the least variance, best

:::::::::::::: {.columns}
::: {.column width="50%"}
$$b^*=\left[A+(X'X)^{-1} X \prime\right] y$$


where A is some nonstochastic k × n matrix, similar to X. Simplifying, we obtain

$$
\begin{aligned}
&b^*=A y+(X'X)^{-1} X'y \\
&=A y+b
\end{aligned}
$$

where b is the least-squares estimator


:::
::: {.column width="50%"}
Now,

$$
\begin{aligned}
&E\left(b^*\right)=\left[A+(X'X)^{-1} X'\right] E(y) \\
&=\left[A+(X'X)^{-1} X \prime\right](X B) \\
&=(A X+I) B
\end{aligned}
$$

Now $E(b*) = B$ if and only if AX = 0. In other words, for the linear estimator b* to be unbiased, AX must be 0.


:::
::::::::::::::


# Have the least variance, best

:::::::::::::: {.columns}
::: {.column width="50%"}

\small

Thus,


$$
\begin{aligned}
b^*&=\left[A+(X'X)^{-1} X \prime\right][X B+u], \text{substituting for } (y) \\
&=B+[A+(X'X) X-1 \prime] u \text{, because } A X=0
\end{aligned}
$$

Given that u has zero mean and constant variance ($=\sigma^2I$), we can now find the variance of b* as follows:


:::
::: {.column width="50%"}

\small

$$
\begin{aligned}
\operatorname{cov}\left(b^*\right)&=E\left[A+(X \prime X)^{-1} X \prime\right] u u \prime\left[A+(X \prime X)^{-1} X^{\prime}\right] \prime \\
&=\left[A+(X \prime X)^{-1} X \prime\right] E(u u \prime)\left[A+(X \prime X)^{-1} X \prime\right] \prime \\
&=\sigma^2\left[A A \prime+(X \prime X)^{-1}\right] \\
&=\sigma^2(X \prime X)^{-1}+A A \prime \sigma^2 \\
&=\operatorname{var}(b)+A A \prime \sigma^2
\end{aligned}
$$

- shows that the covariance matrix of b* is equal to the covariance matrix of b plus a positive semidefinite matrix

:::
::::::::::::::

# Best Linear Unbiased Estimator (BLUE)

1. The parameters are *linear* $$(X'X)^{-1}X'$$
2. The parameters are *unbiased*  $$E(\widehat{\beta}|X) = \beta$$
3. The parameters have the least variance of all unbiased linear estimators, *best*. $$\widehat{\beta}^{*} = var - cov(\widehat{\beta}) + \sigma^2CC'$$


# {.standout}
- ALT-TAB to Excel, linear estimator example

# Conclusion


- We saw a  review of linear regression
- How multivariate regression works
- And how OLS is the best of the linear estimators

# Acknowledgements

:::::::::::::: {.columns}
::: {.column width="50%"}

## Team

- Joshua Pritkin. 
- Rob Kirkpatrick. 

\vspace{3mm}
- Michael C Neale.  

- NIH grant no R01 DA049867 and 5T32MH-020030 


:::
::: {.column width="50%"}


## Contact 


\begin{center}\includegraphics[width=0.8\linewidth]{../graphs/qr-twitter-1} \end{center}


:::
::::::::::::::

\appendix

#  {.standout}


- Thank you

# Interpretation 4


\begin{center}\includegraphics[width=1\linewidth]{../graphs/interpret-4} \end{center}


# What it means to be best

Now that we have shown that the OLS estimator is linear and unbiased, we need to prove that it is also the best linear unbiased estimator.

## What exactly do we mean by best?

When $\widehat{\beta}$ is a scalar (i.e., there is only one regressor), we consider $\widehat{\beta}$ to be the best among those we are considering (i.e., among all the linear unbiased estimators) if and only if it has the smallest possible variance, that is, if its deviations from the true value $\beta$ tend to be the smallest on average. Thus, $\widehat{\beta}$ is the best linear unbiased estimator (BLUE) if and only if

$$Var[\widehat{\beta}|X] \leq Var[\widetilde{\beta}|X]$$

for any other linear unbiased estimator $\widetilde{\beta}$.


# What it means to be best

Since we often deal with more than one regressor, we have to extend this definition to a multivariate context. We do this by requiring that

$$Var[\alpha\widehat{\beta}|X] \leq Var[\alpha\widetilde{\beta}|X]$$

for any $1	\times K$ constant vector $\alpha$, any other linear unbiased estimator $\widetilde{\beta}$.

In other words, OLS is BLUE if and only if any linear combination of the regression coefficients is estimated more precisely by OLS than by any other linear unbiased estimator.

# What it means to be best

Condition (1, previous) is satisfied if and only if

$$Var[\widetilde{\beta}|X] - Var[\widehat{\beta}|X]$$

is a positive semi-definite matrix.


In the next two sections we will derive $Var[\widehat{\beta}|X]$ (the covariance matrix of the OLS estimator), and then we will prove that (2, above) is positive-semidefinite, so that OLS is BLUE.

# The covariance matrix of the OLS estimator

The conditional covariance matrix of the OLS estimator is

$$Var[\widehat{\beta}|X] = \sigma^2(X'X)^{-1}$$

# OLS is BLUE

Since we are considering the set of linear estimators, we can write any estimator in this set as

$$\widetilde{\beta} = Cy$$

where $C$ is a $K	\times N$ matrix.

Furthermore, if we define

$$D = C - (X'X)^{-1}X'$$


# OLS is BLUE

then we can write

$$\begin{aligned}\widetilde{\beta} &= Cy\\
                    &= Dy + (X'X)^{-1}X'y\\
                    &= Dy + \widehat{\beta}
                    \end{aligned}$$

It is possible to prove that $DX=0$ if $\widetilde{\beta }$ is unbiased.




# OLS is BLUE

By using this result, we can also prove that

$$Var[\widehat{\beta}|X] = Var[\widetilde{\beta}|X] + \sigma^2DD'$$

As a consequence,

$$Var[\widehat{\beta}|X] - Var[\widetilde{\beta}|X] + \sigma^2DD'$$

is positive semi-definite because [eq28] is positive semi-definite. This is true for any unbiased linear estimator $widetilde{\beta }$. Therefore, the OLS estimator is BLUE.

